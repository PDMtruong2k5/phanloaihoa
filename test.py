# ==============================
# Nháº­n diá»‡n hoa Oxford Flowers 102 vá»›i PyTorch
# Code gá»‘c Ä‘Æ°á»£c viáº¿t láº¡i, thÃªm chÃº thÃ­ch tiáº¿ng Viá»‡t vÃ  in káº¿t quáº£ tiáº¿ng Viá»‡t
# ==============================

import os, warnings
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, Dataset
from PIL import Image
import pandas as pd
import numpy as np

warnings.filterwarnings('ignore')

# ---------- 1. Cáº¥u hÃ¬nh thiáº¿t bá»‹ ----------
# Kiá»ƒm tra cÃ³ GPU CUDA khÃ´ng, náº¿u cÃ³ thÃ¬ dÃ¹ng, náº¿u khÃ´ng thÃ¬ dÃ¹ng CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ Äang sá»­ dá»¥ng thiáº¿t bá»‹: {device}")

# ---------- 2. ÄÆ°á»ng dáº«n dá»¯ liá»‡u ----------
# ThÆ° má»¥c dá»¯ liá»‡u (sá»­a náº¿u cáº§n)
DATA_DIR = "/kaggle/input/oxford-flowers-102"
TRAIN_DIR = os.path.join(DATA_DIR, "train")
VALID_DIR = os.path.join(DATA_DIR, "valid")
TEST_DIR  = os.path.join(DATA_DIR, "test")

# Kiá»ƒm tra cáº¥u trÃºc thÆ° má»¥c
for path, name in [(TRAIN_DIR, "Train"), (VALID_DIR, "Valid"), (TEST_DIR, "Test")]:
    if os.path.exists(path):
        count = len([f for f in os.listdir(path) if not f.startswith('.')])
        print(f"âœ… ThÆ° má»¥c {name}: cÃ³ {count} má»¥c")
    else:
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c {name}: {path}")

# ---------- 3. Tiá»n xá»­ lÃ½ áº£nh ----------
# Táº­p train: cÃ³ tÄƒng cÆ°á»ng dá»¯ liá»‡u (Data Augmentation)
train_transform = transforms.Compose([
    transforms.Resize(256),                         # Äá»•i kÃ­ch thÆ°á»›c áº£nh
    transforms.RandomResizedCrop(224, scale=(0.8,1.0)), # Cáº¯t ngáº«u nhiÃªn
    transforms.RandomHorizontalFlip(p=0.5),        # Láº­t ngang ngáº«u nhiÃªn
    transforms.RandomRotation(30),                 # Xoay ngáº«u nhiÃªn
    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.4, hue=0.15), # Biáº¿n Ä‘á»•i mÃ u
    transforms.RandomGrayscale(p=0.05),            # Ngáº«u nhiÃªn chuyá»ƒn sang áº£nh xÃ¡m
    transforms.ToTensor(),                         # ÄÆ°a vá» tensor [0,1]
    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])  # Chuáº©n hÃ³a theo ImageNet
])

# Táº­p valid/test: chá»‰ resize vÃ  chuáº©n hÃ³a, khÃ´ng augment
val_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])
])

# ---------- 4. Táº¡o Dataset & DataLoader ----------
# DÃ¹ng ImageFolder cho train/valid
train_ds = datasets.ImageFolder(TRAIN_DIR, transform=train_transform)
valid_ds = datasets.ImageFolder(VALID_DIR, transform=val_transform)

# batch_size lá»›n hÆ¡n khi cÃ³ GPU
batch_size = 32 if device.type == 'cuda' else 16
train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)
valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, num_workers=2)

# Táº¡o Dataset custom cho test (chá»‰ cÃ³ áº£nh, khÃ´ng cÃ³ nhÃ£n)
class TestDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root = root_dir
        self.files = sorted([f for f in os.listdir(root_dir) if f.lower().endswith(('.jpg','.jpeg','.png'))])
        self.transform = transform

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        fname = self.files[idx]
        path = os.path.join(self.root, fname)
        img = Image.open(path).convert("RGB")
        if self.transform:
            img = self.transform(img)
        return img, fname

test_ds = TestDataset(TEST_DIR, transform=val_transform)
test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)

# ThÃ´ng tin dá»¯ liá»‡u
num_classes = len(train_ds.classes)
print(f"ğŸ“Š Train: {len(train_ds)} áº£nh, Valid: {len(valid_ds)} áº£nh, Test: {len(test_ds)} áº£nh")
print(f"ğŸ¯ Sá»‘ lá»›p phÃ¢n loáº¡i: {num_classes}")

# Báº£n Ä‘á»“ ngÆ°á»£c tá»« index -> class
inv_map = {v: int(k) for k,v in train_ds.class_to_idx.items()}
print(f"ğŸ”„ VÃ­ dá»¥ Ã¡nh xáº¡ class: {dict(list(inv_map.items())[:5])}")

# ---------- 5. HÃ m táº¡o mÃ´ hÃ¬nh ----------
def create_model(model_name, num_classes, use_pretrained=True):
    """Táº¡o model vá»›i nhiá»u kiáº¿n trÃºc khÃ¡c nhau"""
    if model_name == 'resnet50':
        try:
            from torchvision.models import ResNet50_Weights
            weights = ResNet50_Weights.DEFAULT if use_pretrained else None
            model = models.resnet50(weights=weights)
            model.fc = nn.Sequential(
                nn.Dropout(0.5),
                nn.Linear(model.fc.in_features, num_classes)
            )
        except:
            model = models.resnet50(weights=None)
            model.fc = nn.Linear(model.fc.in_features, num_classes)

    elif model_name == 'efficientnet':
        try:
            from torchvision.models import EfficientNet_B0_Weights
            weights = EfficientNet_B0_Weights.DEFAULT if use_pretrained else None
            model = models.efficientnet_b0(weights=weights)
            model.classifier = nn.Sequential(
                nn.Dropout(0.5),
                nn.Linear(model.classifier[1].in_features, num_classes)
            )
        except:
            # fallback vá» resnet34 náº¿u khÃ´ng cÃ³ EfficientNet
            model = models.resnet34(weights=None)
            model.fc = nn.Linear(model.fc.in_features, num_classes)

    else:  # máº·c Ä‘á»‹nh dÃ¹ng resnet18
        try:
            from torchvision.models import ResNet18_Weights
            weights = ResNet18_Weights.DEFAULT if use_pretrained else None
            model = models.resnet18(weights=weights)
        except:
            model = models.resnet18(weights=None)
            use_pretrained = False

        # Thay fc báº±ng classifier má»›i
        model.fc = nn.Sequential(
            nn.Dropout(0.4),
            nn.Linear(model.fc.in_features, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    return model, use_pretrained

# Chá»n model theo thiáº¿t bá»‹
if device.type == 'cuda':
    model_name = 'resnet50'
else:
    model_name = 'resnet18'

model, has_pretrained = create_model(model_name, num_classes, use_pretrained=True)
model = model.to(device)
print(f"ğŸ”¥ Sá»­ dá»¥ng {model_name} {'(cÃ³ pretrained)' if has_pretrained else '(train tá»« Ä‘áº§u)'}")

# ---------- 6. Thiáº¿t láº­p huáº¥n luyá»‡n ----------
criterion = nn.CrossEntropyLoss()

if has_pretrained:
    # ÄÃ³ng bÄƒng cÃ¡c layer sá»›m, chá»‰ fine-tune layer cuá»‘i
    for name, param in model.named_parameters():
        if 'fc' not in name and 'classifier' not in name:
            if 'layer4' not in name and 'features.7' not in name:
                param.requires_grad = False

    optimizer = optim.Adam([
        {'params': [p for n,p in model.named_parameters() if 'fc' in n or 'classifier' in n], 'lr':1e-3},
        {'params': [p for n,p in model.named_parameters() if ('layer4' in n or 'features.7' in n) and 'fc' not in n], 'lr':1e-4}
    ], weight_decay=1e-4)
else:
    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)

scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)

# ---------- 7. VÃ²ng láº·p huáº¥n luyá»‡n ----------
best_val_acc = 0.0
patience_counter = 0
max_patience = 7

EPOCHS = 25 if device.type == 'cuda' else 15
print(f"ğŸš€ Báº¯t Ä‘áº§u huáº¥n luyá»‡n tá»‘i Ä‘a {EPOCHS} epochs...")

for epoch in range(EPOCHS):
    # Huáº¥n luyá»‡n
    model.train()
    running_loss = 0.0
    total, correct = 0, 0
    for imgs, labels in train_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()*imgs.size(0)
        preds = outputs.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    train_loss = running_loss/total
    train_acc = correct/total

    # Validation
    model.eval()
    v_total, v_correct = 0, 0
    with torch.no_grad():
        for imgs, labels in valid_loader:
            imgs, labels = imgs.to(device), labels.to(device)
            outputs = model(imgs)
            preds = outputs.argmax(dim=1)
            v_correct += (preds == labels).sum().item()
            v_total += labels.size(0)
    val_acc = v_correct/v_total

    # LÆ°u model tá»‘t nháº¥t
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), "best_model.pth")
        patience_counter = 0
        print(f"ğŸ‰ Epoch {epoch+1}: Cáº£i thiá»‡n! Train acc={train_acc:.3f}, Val acc={val_acc:.3f}")
    else:
        patience_counter += 1
        print(f"ğŸ“Š Epoch {epoch+1}: Train acc={train_acc:.3f}, Val acc={val_acc:.3f} (patience={patience_counter})")

    scheduler.step(val_acc)

    # Early stopping
    if patience_counter >= max_patience:
        print(f"â¹ï¸ Dá»«ng sá»›m táº¡i epoch {epoch+1}")
        break

print(f"ğŸ† Äá»™ chÃ­nh xÃ¡c cao nháº¥t trÃªn táº­p Validation: {best_val_acc:.4f}")

# ---------- 8. Dá»± Ä‘oÃ¡n trÃªn táº­p Test ----------
if os.path.exists("best_model.pth"):
    model.load_state_dict(torch.load("best_model.pth", map_location=device))
    print("âœ… ÄÃ£ load model tá»‘t nháº¥t")

model.eval()
ids, classes_out = [], []
print("ğŸ”® Äang dá»± Ä‘oÃ¡n trÃªn táº­p Test...")

with torch.no_grad():
    for imgs, fnames in test_loader:
        imgs = imgs.to(device)
        outputs = model(imgs)
        preds = outputs.argmax(dim=1).cpu().numpy()
        for p, fname in zip(preds, fnames):
            class_id = inv_map[int(p)]
            ids.append(fname)
            classes_out.append(int(class_id))

# ---------- 9. Táº¡o file ná»™p ----------
submission_df = pd.DataFrame({
    "id": ids,
    "class": classes_out
})

print(f"ğŸ“‹ KÃ­ch thÆ°á»›c submission: {submission_df.shape}")
print(f"ğŸ”¢ Khoáº£ng class dá»± Ä‘oÃ¡n: {submission_df['class'].min()} - {submission_df['class'].max()}")
print("ğŸ“„ Xem vÃ i dÃ²ng Ä‘áº§u cá»§a submission:")
print(submission_df.head())

submission_df.to_csv("submission.csv", index=False)
submission_df.to_csv("/kaggle/working/submission.csv", index=False)

print("ğŸ’¾ ÄÃ£ lÆ°u file submission.csv thÃ nh cÃ´ng!")
print(f"âœ… Sáºµn sÃ ng ná»™p vá»›i {len(submission_df)} dá»± Ä‘oÃ¡n")
